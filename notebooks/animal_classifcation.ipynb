{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c74095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f741a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317b41dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset split completed!\n"
     ]
    }
   ],
   "source": [
    "# split dataset (one time operation)\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = '../data/animals_dataset/raw-img/'\n",
    "output_path = '../data/animals_dataset_reduced/'\n",
    "\n",
    "# Define the percentage of images to keep\n",
    "sample_ratio = 0.05\n",
    "\n",
    "# Define split ratios for the reduced dataset\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Create output directories\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_path = os.path.join(output_path, split)\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "# Process each class folder\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_folder = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_folder):\n",
    "        # Get all image file paths for the class\n",
    "        image_files = [os.path.join(class_folder, img) for img in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, img))]\n",
    "        \n",
    "        # Randomly sample 25% of the images\n",
    "        sampled_files, _ = train_test_split(image_files, test_size=(1 - sample_ratio), random_state=42)\n",
    "        \n",
    "        # Split the sampled files into train, val, and test\n",
    "        train_files, temp_files = train_test_split(sampled_files, test_size=(1 - train_ratio), random_state=42)\n",
    "        val_files, test_files = train_test_split(temp_files, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "        \n",
    "        # Copy files to respective directories\n",
    "        for split, files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
    "            split_class_folder = os.path.join(output_path, split, class_name)\n",
    "            os.makedirs(split_class_folder, exist_ok=True)\n",
    "            for file in files:\n",
    "                shutil.copy(file, os.path.join(split_class_folder, os.path.basename(file)))\n",
    "\n",
    "print(\"Reduced dataset split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b739da",
   "metadata": {},
   "source": [
    "# Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f26b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and augmentation\n",
    "from torchvision import transforms\n",
    "\n",
    "# Basic transforms for baseline (minimal augmentation)\n",
    "baseline_transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Standard ImageNet size\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "baseline_transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        for label, class_name in enumerate(os.listdir(root_dir)):\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for img_file in os.listdir(class_folder):\n",
    "                    img_path = os.path.join(class_folder, img_file)\n",
    "                    if os.path.isfile(img_path):\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98ad7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = '../data/animals_dataset_reduced/train'\n",
    "val_dir = '../data/animals_dataset_reduced/val'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AnimalDataset(root_dir=train_dir, transform=baseline_transform_train)\n",
    "val_dataset = AnimalDataset(root_dir=val_dir, transform=baseline_transform_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137eafc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tajvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tajvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained resnet-18 model\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pretrained ResNet-18 model\n",
    "# Load the pretrained MobileNetV2 model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the classifier layer to match the number of classes\n",
    "num_classes = 10  # Update this based on your dataset\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09e40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d419eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config\n",
    "# Define training parameters\n",
    "num_epochs = 10\n",
    "train_loader = train_loader  # Use the DataLoader defined earlier\n",
    "val_loader = val_loader      # Use the DataLoader defined earlier\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c9f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tajvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/331 [00:00<?, ?it/s]C:\\Users\\tajvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\tajvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "class TrainingLogger:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "    \n",
    "    def log_epoch(self, train_loss, val_loss, train_acc, val_acc, lr):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(self.train_losses, label='Train Loss')\n",
    "        axes[0].plot(self.val_losses, label='Val Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(self.train_accuracies, label='Train Acc')\n",
    "        axes[1].plot(self.val_accuracies, label='Val Acc')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].set_title('Training and Validation Accuracy')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('baseline_training_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def run_baseline_experiment(config):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_loader = config['train_loader']\n",
    "    val_loader = config['val_loader']\n",
    "    \n",
    "    # Create model\n",
    "    model = models.resnet18(pretrained=config['pretrained'])\n",
    "    model.fc = nn.Linear(model.fc.in_features, config['num_classes'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config['learning_rate'],\n",
    "                           weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # Training logger\n",
    "    logger = TrainingLogger()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "        \n",
    "        start_time = time.time()  # Start timing\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, \n",
    "                                                criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time  # End timing\n",
    "        print(f\"Epoch {epoch+1} took {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        logger.log_epoch(train_loss, val_loss, train_acc, val_acc, \n",
    "                         config['learning_rate'])\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'baseline_best_model.pth')\n",
    "            print(f\"Saved best model with validation accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Plot results\n",
    "    logger.plot_metrics()\n",
    "    \n",
    "    return model, logger\n",
    "\n",
    "# Example configuration\n",
    "config = {\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'num_classes': 10,\n",
    "    'pretrained': True,\n",
    "    'learning_rate': 1,#0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_epochs': 5\n",
    "}\n",
    "\n",
    "# Run the experiment\n",
    "model, logger = run_baseline_experiment(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
